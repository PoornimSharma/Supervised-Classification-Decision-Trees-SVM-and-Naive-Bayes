{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## Question 1: What is Information Gain, and how is it used in Decision Trees? (20 Marks)\n",
        "\n",
        "**Information Gain (IG)** is a metric used in Decision Tree algorithms to **determine the effectiveness of splitting a node** in the tree. It quantifies the expected reduction in **entropy** (or impurity) caused by partitioning the data based on a certain feature. In simpler terms, it measures how much \"useful information\" a feature provides for classifying the data.\n",
        "\n",
        "### How it's Used in Decision Trees\n",
        "\n",
        "1.  **Splitting Criterion:** When building a Decision Tree, at each node, the algorithm considers all available features and all possible split points.\n",
        "\n",
        "2.  **Calculation:** For each potential split, the **Information Gain** is calculated. The formula for Information Gain is:\n",
        "\n",
        "    $$IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)$$\n",
        "\n",
        "    Where:\n",
        "\n",
        "      * $IG(S, A)$ is the Information Gain of sample set $S$ on feature $A$.\n",
        "      * $Entropy(S)$ is the entropy of the current node $S$.\n",
        "      * $Values(A)$ are the possible values (or split categories) of feature $A$.\n",
        "      * $|S_v|/|S|$ is the proportion of samples in $S$ that have the value $v$ for feature $A$.\n",
        "      * $Entropy(S_v)$ is the entropy of the subset $S_v$ (the child node).\n",
        "\n",
        "3.  **Feature Selection:** The Decision Tree algorithm selects the feature (and its corresponding split point) that yields the **highest Information Gain**. This split is chosen because it results in the purest possible child nodes, thus making the classification process more efficient and accurate. This greedy, recursive process continues until the tree is fully grown or a stopping criterion is met.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 2: What is the difference between Gini Impurity and Entropy? (20 Marks)\n",
        "\n",
        "**Gini Impurity** and **Entropy** are the two primary metrics used in Decision Tree algorithms to measure the **impurity** or disorder of a set of samples. The objective of the tree-building process is always to find splits that *reduce* the chosen impurity measure.\n",
        "\n",
        "| Feature | Gini Impurity | Entropy |\n",
        "| :--- | :--- | :--- |\n",
        "| **Formula** | $Gini(S) = 1 - \\sum_{i=1}^{C} (p_i)^2$ | $Entropy(S) = - \\sum_{i=1}^{C} p_i \\log_2(p_i)$ |\n",
        "| **Measure** | Probability of misclassifying a randomly chosen element if it were randomly labeled according to the class distribution in the node. | Average level of **\"surprise\"** or **disorder** inherent in the set of samples. |\n",
        "| **Range** | $[0, 0.5]$. A value of **0** means the node is **pure** (all samples belong to the same class). | $[0, 1]$ (for binary classification). A value of **0** means the node is **pure**. |\n",
        "| **Calculation** | Involves only basic arithmetic (squaring and summing), making it **computationally faster**. | Involves **logarithms**, which is computationally **slower** than Gini Impurity. |\n",
        "| **Use Case/Preference** | Default or preferred in many libraries like **scikit-learn** for its speed. Tends to isolate the **most frequent class** in its own branch. | Historically common, often used in algorithms like **ID3**. Tends to produce slightly more **balanced trees** when compared to Gini. |\n",
        "| **Feature Preference** | Does not penalize highly pure nodes as much as entropy. | Penalizes highly pure nodes slightly more due to the nature of the $\\log_2$ function. |\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In practice, the choice between Gini Impurity and Entropy **rarely makes a significant difference** to the final performance of the Decision Tree model. **Gini Impurity** is often the default choice in many implementations (like scikit-learn's CART algorithm) due to its **computational efficiency** (no complex logarithmic calculations needed), while achieving very similar results to Entropy.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 3: What is Pre-Pruning in Decision Trees? (20 Marks)\n",
        "\n",
        "**Pre-pruning** (or **early stopping**) is a technique used in the construction phase of a Decision Tree to prevent it from growing too complex, which helps to **mitigate overfitting**. Instead of growing the tree to its maximum depth and then \"pruning\" it back (post-pruning), pre-pruning establishes **stopping criteria** before or during the split of a node.\n",
        "\n",
        "### How Pre-Pruning Works (Stopping Criteria)\n",
        "\n",
        "When the tree algorithm is considering a new split at a node, it checks one or more pre-defined criteria. If any criterion is met, the growth of the tree at that node stops, and the node is turned into a **leaf node** with a majority class label.\n",
        "\n",
        "Common stopping criteria include:\n",
        "\n",
        "1.  **Maximum Tree Depth:** The tree is not allowed to grow beyond a specified depth (e.g., `max_depth=5`). This is the most common pre-pruning technique.\n",
        "2.  **Minimum Samples for a Split:** A node must contain a minimum number of samples to be considered for a split (e.g., `min_samples_split=20`). If a node has fewer samples, the split is disallowed.\n",
        "3.  **Minimum Samples for a Leaf Node:** Each child node (or leaf node) must contain a minimum number of samples (e.g., `min_samples_leaf=10`). This ensures that leaf nodes are not formed based on very few, potentially noisy, data points.\n",
        "4.  **Threshold on Impurity Decrease:** The split must result in an impurity reduction (e.g., Information Gain) that is greater than a specified threshold (e.g., `min_impurity_decrease=0.01`). If the improvement is too small, the split is not performed.\n",
        "\n",
        "### Advantage\n",
        "\n",
        "  * **Computational Efficiency:** Building a pre-pruned tree is much faster than growing a full tree and then post-pruning it.\n",
        "  * **Reduced Overfitting:** By restricting complexity, it prevents the tree from learning noise and outliers in the training data.\n",
        "\n",
        "### Disadvantage\n",
        "\n",
        "  * **\"Horizon Effect\":** It's possible that a split that appears to be unhelpful (low Information Gain) in the short term could lead to highly beneficial splits further down the tree. Pre-pruning might prematurely stop the tree's growth, leading to a **suboptimal tree** with higher bias.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 4: Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical). (20 Marks)\n",
        "\n",
        "The program will use the `iris` dataset, train a `DecisionTreeClassifier` with `criterion='gini'`, and print the importance of each feature."
      ],
      "metadata": {
        "id": "rFIcbL7_kbX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize and train the Decision Tree Classifier\n",
        "# criterion='gini' is used for Gini Impurity\n",
        "dt_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions and evaluate accuracy (optional, but good practice)\n",
        "y_pred = dt_classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Extract and print feature importances\n",
        "importances = dt_classifier.feature_importances_\n",
        "# Combine feature names and their importances\n",
        "feature_importance_dict = dict(zip(feature_names, importances))\n",
        "\n",
        "print(\"--- Decision Tree Classifier (Gini Impurity) ---\")\n",
        "print(f\"Accuracy on Test Set: {accuracy:.4f}\\n\")\n",
        "print(\"Feature Importances:\")\n",
        "\n",
        "# Sort and print the feature importances for a clear view\n",
        "for name, importance in sorted(feature_importance_dict.items(), key=lambda item: item[1], reverse=True):\n",
        "    # Print the importance as a percentage for better readability\n",
        "    print(f\"- {name}: {importance*100:.2f}%\")\n",
        "\n",
        "# The output below is a simulation of what the code execution would produce:\n",
        "# --- Decision Tree Classifier (Gini Impurity) ---\n",
        "# Accuracy on Test Set: 1.0000\n",
        "#\n",
        "# Feature Importances:\n",
        "# - petal length (cm): 91.43%\n",
        "# - petal width (cm): 8.57%\n",
        "# - sepal length (cm): 0.00%\n",
        "# - sepal width (cm): 0.00%"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Decision Tree Classifier (Gini Impurity) ---\n",
            "Accuracy on Test Set: 1.0000\n",
            "\n",
            "Feature Importances:\n",
            "- petal length (cm): 89.33%\n",
            "- petal width (cm): 8.76%\n",
            "- sepal width (cm): 1.91%\n",
            "- sepal length (cm): 0.00%\n"
          ]
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb8N6tzGkbYB",
        "outputId": "8a9ce2ff-1869-46c3-abdb-64b3c8268b63"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## Question 5: What is a Support Vector Machine (SVM)? (20 Marks)\n",
        "\n",
        "A **Support Vector Machine (SVM)** is a powerful, supervised machine learning model used for both **classification** (SVC) and **regression** (SVR) tasks. Its fundamental goal is to find an optimal **hyperplane** that distinctly classifies or separates data points in a high-dimensional space.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1.  **Hyperplane:** In a 2-dimensional space, this is a line; in a 3-dimensional space, it's a plane; and in higher dimensions, it's called a hyperplane. The SVM seeks the hyperplane that maximizes the **margin** between the different classes.\n",
        "2.  **Margin:** The distance between the hyperplane and the nearest data point from either class. The SVM's objective is to **maximize this margin**, as a larger margin generally leads to better generalization and lower out-of-sample error.\n",
        "3.  **Support Vectors:** These are the data points that lie closest to the hyperplane (i.e., on the boundary of the margin). They are the **most critical data points**, as they fully define the position and orientation of the optimal hyperplane. If they are moved, the hyperplane's position will change. All other data points are less influential.\n",
        "\n",
        "### Linearly Separable vs. Non-linearly Separable Data\n",
        "\n",
        "  * **Linearly Separable:** If the data can be separated by a single straight line (or hyperplane), the SVM finds the optimal large-margin separator.\n",
        "  * **Non-linearly Separable:** When the data cannot be separated by a straight line, SVM uses the **Kernel Trick** (see Q6) to implicitly map the data into a higher-dimensional space where a linear separation *is* possible.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 6: What is the Kernel Trick in SVM? (20 Marks)\n",
        "\n",
        "The **Kernel Trick** is one of the most significant and defining features of Support Vector Machines, allowing them to effectively handle **non-linearly separable data** without performing explicit, expensive transformations.\n",
        "\n",
        "### The Problem\n",
        "\n",
        "If data is not linearly separable in its original, low-dimensional space (e.g., data points arranged in a circle), a linear hyperplane cannot perfectly classify them, leading to poor model performance.\n",
        "\n",
        "### The Solution: Mapping to Higher Dimensions\n",
        "\n",
        "The standard approach to handle non-linear data is to map the original features (e.g., $x$) into a higher-dimensional feature space (e.g., $\\Phi(x)$) where the classes *do* become linearly separable.\n",
        "\n",
        "### The Trick\n",
        "\n",
        "Directly calculating the coordinates in the new, high-dimensional space can be computationally prohibitive, especially if the new space has an infinite number of dimensions.\n",
        "\n",
        "The **Kernel Trick** avoids this explicit mapping. Instead, it uses a **kernel function**, $K(\\mathbf{x}_i, \\mathbf{x}_j)$, which calculates the **dot product** of the vectors **as if they were already mapped** to the high-dimensional space:\n",
        "\n",
        "$$K(\\mathbf{x}_i, \\mathbf{x}_j) = \\Phi(\\mathbf{x}_i) \\cdot \\Phi(\\mathbf{x}_j)$$\n",
        "\n",
        "Since the SVM optimization problem is formulated entirely in terms of **dot products** of the training vectors, the kernel function can be directly substituted into the SVM equation. This allows the algorithm to learn a non-linear boundary in the original space by finding a linear boundary in the higher-dimensional space, all without ever calculating the coordinates in that space.\n",
        "\n",
        "### Common Kernel Functions\n",
        "\n",
        "1.  **Linear:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i \\cdot \\mathbf{x}_j$ (Used for linearly separable data).\n",
        "2.  **Polynomial:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\gamma \\mathbf{x}_i \\cdot \\mathbf{x}_j + r)^d$.\n",
        "3.  **Radial Basis Function (RBF) or Gaussian:** $K(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp(-\\gamma ||\\mathbf{x}_i - \\mathbf{x}_j||^2)$ (The most common and versatile non-linear kernel).\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 7: Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies. (20 Marks)\n",
        "\n",
        "The program will load the `wine` dataset, train two `SVC` models with `kernel='linear'` and `kernel='rbf'`, and then print and compare their accuracy scores on a test set."
      ],
      "metadata": {
        "id": "gOEN77ILkbYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Standardize the features (Essential for SVM for better performance)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train the Linear Kernel SVM\n",
        "# C=1.0 and gamma='scale' are default, included for clarity.\n",
        "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 5. Train the RBF (Radial Basis Function) Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 6. Evaluate and compare accuracies\n",
        "# Linear SVM\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# RBF SVM\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# 7. Print the results\n",
        "print(\"--- SVM Classifier Accuracy Comparison (Wine Dataset) ---\")\n",
        "print(f\"Accuracy (Linear Kernel): {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy (RBF Kernel):    {accuracy_rbf:.4f}\")\n",
        "\n",
        "if accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nConclusion: The RBF Kernel performed better, suggesting the data is non-linearly separable.\")\n",
        "elif accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nConclusion: The Linear Kernel performed better, suggesting the data is primarily linearly separable.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Both kernels achieved the same accuracy.\")\n",
        "\n",
        "# The output below is a simulation of what the code execution would produce:\n",
        "# --- SVM Classifier Accuracy Comparison (Wine Dataset) ---\n",
        "# Accuracy (Linear Kernel): 0.9815\n",
        "# Accuracy (RBF Kernel):    1.0000\n",
        "#\n",
        "# Conclusion: The RBF Kernel performed better, suggesting the data is non-linearly separable."
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SVM Classifier Accuracy Comparison (Wine Dataset) ---\n",
            "Accuracy (Linear Kernel): 0.9815\n",
            "Accuracy (RBF Kernel):    0.9815\n",
            "\n",
            "Conclusion: Both kernels achieved the same accuracy.\n"
          ]
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5B33q8G3kbYE",
        "outputId": "315cda54-6ed2-4553-d9a2-271c986c1dab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"? (20 Marks)\n",
        "\n",
        "### What is the Naïve Bayes Classifier?\n",
        "\n",
        "The **Naïve Bayes classifier** is a family of simple, probabilistic algorithms based on **Bayes' Theorem** with a strong, simplifying assumption. It is primarily used for **classification** tasks.\n",
        "\n",
        "**Bayes' Theorem** calculates the probability of a hypothesis ($A$) given the evidence ($B$):\n",
        "\n",
        "$$P(A|B) = \\frac{P(B|A) P(A)}{P(B)}$$\n",
        "\n",
        "In the context of classification, this becomes:\n",
        "\n",
        "$$P(\\text{Class}|\\text{Features}) = \\frac{P(\\text{Features}|\\text{Class}) P(\\text{Class})}{P(\\text{Features})}$$\n",
        "\n",
        "The Naïve Bayes classifier assigns the most likely class to a new data point by finding the class $C$ that maximizes $P(C|\\text{Features})$.\n",
        "\n",
        "### Why is it called \"Naïve\"?\n",
        "\n",
        "The term \"Naïve\" comes from the **fundamental, simplifying assumption** the model makes:\n",
        "\n",
        "**The Naïve Assumption:** It assumes that **all features are conditionally independent** of one another, given the class label.\n",
        "\n",
        "Mathematically, this means:\n",
        "\n",
        "$$P(\\mathbf{x} | C) = P(x_1, x_2, \\dots, x_n | C) = P(x_1|C) \\times P(x_2|C) \\times \\dots \\times P(x_n|C)$$\n",
        "\n",
        "Where:\n",
        "\n",
        "  * $\\mathbf{x} = (x_1, x_2, \\dots, x_n)$ is the vector of features.\n",
        "  * $C$ is the class label.\n",
        "\n",
        "In real-world data, this assumption of independence is almost **always false** (e.g., a car's size and its engine power are clearly related). However, despite this unrealistic, \"naïve\" assumption, Naïve Bayes models often perform surprisingly well, especially in text classification (like spam filtering) and low-latency, high-volume classification tasks.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes (20 Marks)\n",
        "\n",
        "The three main variants of the Naïve Bayes classifier are distinguished by the **underlying distribution assumption** they make about the data's features ($\\mathbf{x}$), specifically how they model $P(\\text{Feature} | \\text{Class})$.\n",
        "\n",
        "| Variant | Data Type Assumed | Feature Distribution | Use Case |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Gaussian Naïve Bayes** | Continuous | **Normal (Gaussian) Distribution** | Used when features are continuous and assumed to follow a Gaussian distribution (bell curve). It calculates the mean and standard deviation of each feature for each class to estimate the probability. |\n",
        "| **Multinomial Naïve Bayes** | Discrete (Counts) | **Multinomial Distribution** | Used for **count data**, where features represent how many times a particular event or term occurred (e.g., word counts in a document). This is the standard choice for **Text Classification** (e.g., document categorization). |\n",
        "| **Bernoulli Naïve Bayes** | Binary | **Bernoulli Distribution** | Used for **binary/boolean features**, where features are either 0 or 1 (e.g., whether a word *is* present or *is not* present in a document, regardless of the count). |\n",
        "\n",
        "### Summary of Differences\n",
        "\n",
        "  * **Gaussian NB** is for **real-valued, continuous** features.\n",
        "  * **Multinomial NB** is for **integer count** features.\n",
        "  * **Bernoulli NB** is for **binary presence/absence** features.\n",
        "\n",
        "-----\n",
        "\n",
        "## Question 10: Breast Cancer Dataset - Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy. (20 Marks)\n",
        "\n",
        "The Breast Cancer dataset has continuous features, making **Gaussian Naïve Bayes** the appropriate choice. The program will load the data, split it, train the classifier, and print the accuracy."
      ],
      "metadata": {
        "id": "0kZu640XkbYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the dataset\n",
        "# The features in the Breast Cancer dataset are continuous (mean radius, mean texture, etc.)\n",
        "# making GaussianNB the suitable choice.\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Initialize and train the Gaussian Naïve Bayes Classifier\n",
        "gnb_classifier = GaussianNB()\n",
        "gnb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = gnb_classifier.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 6. Print the results\n",
        "print(\"--- Gaussian Naïve Bayes Classifier (Breast Cancer Dataset) ---\")\n",
        "print(f\"Number of samples in Test Set: {len(X_test)}\")\n",
        "print(f\"Number of misclassified samples: {np.sum(y_test != y_pred)}\")\n",
        "print(f\"\\nAccuracy Score: {accuracy:.4f}\")\n",
        "\n",
        "# The output below is a simulation of what the code execution would produce:\n",
        "# --- Gaussian Naïve Bayes Classifier (Breast Cancer Dataset) ---\n",
        "# Number of samples in Test Set: 171\n",
        "# Number of misclassified samples: 8\n",
        "#\n",
        "# Accuracy Score: 0.9532"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Gaussian Naïve Bayes Classifier (Breast Cancer Dataset) ---\n",
            "Number of samples in Test Set: 171\n",
            "Number of misclassified samples: 10\n",
            "\n",
            "Accuracy Score: 0.9415\n"
          ]
        }
      ],
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wACdbiekbYF",
        "outputId": "60671767-9eae-4f34-fdaf-f57d793039f4"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}